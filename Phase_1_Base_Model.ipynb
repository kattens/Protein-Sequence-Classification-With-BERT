{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "# install this module for extracting info from fas file instead of doing by hand\n",
        "!pip install biopython"
      ],
      "metadata": {
        "id": "-Zt67QDzbcVI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "c-mDXXQUnAen"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset comprises a folder containing ten FASTA files, each file consisting of numerous sequence names referred to as \"sequences.\" In addition to the existing data, we are introducing an additional class as a label. This label will be represented by the file names, indicating the family to which the various sequences belong."
      ],
      "metadata": {
        "id": "ry_YA5rKnLU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceBuilder:\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "    def sequence_builder(self):\n",
        "        sequences = {}\n",
        "        data = []\n",
        "\n",
        "        for filename in os.listdir(self.folder_path):\n",
        "            file_path = os.path.join(self.folder_path, filename)\n",
        "            if os.path.isfile(file_path):  # Exclude directories\n",
        "                for record in SeqIO.parse(file_path, \"fasta\"):\n",
        "                    sequence_name = record.id\n",
        "                    sequence = str(record.seq)\n",
        "                    sequences[sequence_name] = [sequence, filename]\n",
        "\n",
        "        # Build a dataframe\n",
        "        for key, value in sequences.items():\n",
        "            data.append([key, value[0], value[1]])\n",
        "        df = pd.DataFrame(data, columns=['seq name', 'seq', 'class or file name'])\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "JB7hEW9inDOU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build a dataframe# Example usage\n",
        "folder_path =  \"/content/drive/MyDrive/cs612_sequences\"\n",
        "builder = SequenceBuilder(folder_path)\n",
        "df = builder.sequence_builder()\n",
        "#make sure to change the directory before saving or it will go exacly to the fas file location\n",
        "#df.to_csv('Sequence DataFrame.csv', index = False)"
      ],
      "metadata": {
        "id": "YVQWSfhqnQgJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the sequence DataFrame from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Sequence DataFrame.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the sequences\n",
        "train_encodings = tokenizer(train_df['seq'].tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_df['seq'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Prepare the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df['class or file name'].tolist())\n",
        "val_labels = label_encoder.transform(val_df['class or file name'].tolist())"
      ],
      "metadata": {
        "id": "aZwiBvV8n3t9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "skJwf5q9t4U_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the dataset\n",
        "train_dataset = SequenceDataset(train_encodings, train_labels)\n",
        "val_dataset = SequenceDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "pu5bwmMft54i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iQTNViC2uAQA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create an instance of the model and move it to the device\n",
        "model = MyModel(num_classes=len(label_encoder.classes_)).to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLxiwk9VuE93",
        "outputId": "a16785bc-a9d9-48bf-b469-4cbd5f0f5d5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7swO7d7NbKFI",
        "outputId": "bcde9841-e79d-49f0-e8f1-a598ac74a5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Train Loss: 2.3023 | Train Acc: 15.49%\n",
            "Validation Loss: 2.2829 | Validation Acc: 13.24%\n",
            "Epoch 2/10:\n",
            "Train Loss: 2.2923 | Train Acc: 16.30%\n",
            "Validation Loss: 2.3052 | Validation Acc: 13.24%\n",
            "Epoch 3/10:\n",
            "Train Loss: 2.2575 | Train Acc: 15.71%\n",
            "Validation Loss: 2.2865 | Validation Acc: 13.24%\n",
            "Epoch 4/10:\n",
            "Train Loss: 2.2622 | Train Acc: 16.96%\n",
            "Validation Loss: 2.2609 | Validation Acc: 18.82%\n",
            "Epoch 5/10:\n",
            "Train Loss: 2.2517 | Train Acc: 17.48%\n",
            "Validation Loss: 2.2906 | Validation Acc: 11.76%\n",
            "Epoch 6/10:\n",
            "Train Loss: 2.2640 | Train Acc: 16.22%\n",
            "Validation Loss: 2.3038 | Validation Acc: 13.24%\n",
            "Epoch 7/10:\n",
            "Train Loss: 2.2552 | Train Acc: 15.93%\n",
            "Validation Loss: 2.2553 | Validation Acc: 18.82%\n",
            "Epoch 8/10:\n",
            "Train Loss: 2.2525 | Train Acc: 17.40%\n",
            "Validation Loss: 2.2665 | Validation Acc: 13.24%\n",
            "Epoch 9/10:\n",
            "Train Loss: 2.2507 | Train Acc: 16.37%\n",
            "Validation Loss: 2.3115 | Validation Acc: 13.24%\n",
            "Epoch 10/10:\n",
            "Train Loss: 2.2695 | Train Acc: 16.45%\n",
            "Validation Loss: 2.2655 | Validation Acc: 13.24%\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2%}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_accuracy:.2%}\")\n"
      ]
    }
  ]
}