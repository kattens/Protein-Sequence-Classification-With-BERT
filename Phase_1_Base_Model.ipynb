{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "# install this module for extracting info from fas file instead of doing by hand\n",
        "!pip install biopython"
      ],
      "metadata": {
        "id": "-Zt67QDzbcVI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
      ],
      "metadata": {
        "id": "c-mDXXQUnAen"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset comprises a folder containing ten FASTA files, each file consisting of numerous sequence names referred to as \"sequences.\" In addition to the existing data, we are introducing an additional class as a label. This label will be represented by the file names, indicating the family to which the various sequences belong."
      ],
      "metadata": {
        "id": "ry_YA5rKnLU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceBuilder:\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "    def sequence_builder(self):\n",
        "        sequences = {}\n",
        "        data = []\n",
        "\n",
        "        for filename in os.listdir(self.folder_path):\n",
        "            file_path = os.path.join(self.folder_path, filename)\n",
        "            if os.path.isfile(file_path):  # Exclude directories\n",
        "                for record in SeqIO.parse(file_path, \"fasta\"):\n",
        "                    sequence_name = record.id\n",
        "                    sequence = str(record.seq)\n",
        "                    sequences[sequence_name] = [sequence, filename]\n",
        "\n",
        "        # Build a dataframe\n",
        "        for key, value in sequences.items():\n",
        "            data.append([key, value[0], value[1]])\n",
        "        df = pd.DataFrame(data, columns=['seq name', 'seq', 'class or file name'])\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "JB7hEW9inDOU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build a dataframe# Example usage\n",
        "folder_path =  \"/content/drive/MyDrive/cs612_sequences\"\n",
        "builder = SequenceBuilder(folder_path)\n",
        "df = builder.sequence_builder()\n",
        "#make sure to change the directory before saving or it will go exacly to the fas file location\n",
        "#df.to_csv('Sequence DataFrame.csv', index = False)"
      ],
      "metadata": {
        "id": "YVQWSfhqnQgJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reads the file as a csv. if we ever wanted to change anything about the csv, we have to add it here\n",
        "class CSVHandler():\n",
        "  def __init__(self,file_path):\n",
        "    self.file_path = file_path\n",
        "\n",
        "  def read_csv(self, file_path):\n",
        "     df = pd.read_csv(self.file_path)\n",
        "     return df\n"
      ],
      "metadata": {
        "id": "83cCGHUEDGR6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if you what to remove dashes in your file\n",
        "\"\"\"file_path = '/content/drive/MyDrive/new_dataset.csv'\n",
        "csvhandler = CSVHandler(file_path)\n",
        "\n",
        "#the dataframe that will be used:\n",
        "df = csvhandler.read_csv(file_path)\n",
        "\n",
        "# Specify the column name that contains the \"-\" characters\n",
        "column_name = 'seq'\n",
        "\n",
        "# Remove the \"-\" character from each element in the column\n",
        "df[column_name] = df[column_name].str.replace('-', '')\n",
        "\n",
        "# Save the modified dataset to a new file\n",
        "df.to_csv('modified_dataset.csv', index=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MhyTEBdlSiOZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/new_dataset.csv'\n",
        "csvhandler = CSVHandler(file_path)\n",
        "\n",
        "#the dataframe that will be used:\n",
        "df = csvhandler.read_csv(file_path)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize and encode the sequences\n",
        "train_encodings = tokenizer(train_df['seq'].tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_df['seq'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Prepare the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df['class or file name'].tolist())\n",
        "val_labels = label_encoder.transform(val_df['class or file name'].tolist())\n"
      ],
      "metadata": {
        "id": "aZwiBvV8n3t9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map encoded labels back to original classes\n",
        "train_class_names = label_encoder.inverse_transform(train_labels)\n",
        "val_class_names = label_encoder.inverse_transform(val_labels)\n",
        "\"\"\"\n",
        "# Print the mapping\n",
        "for label, class_name in zip(train_labels, train_class_names):\n",
        "    print(f\"Encoded label: {label}, Class name: {class_name}\")\n",
        "\"\"\"# Create a set to store unique labels and class names\n",
        "unique_labels = set()\n",
        "for label, class_name in zip(train_labels, train_class_names):\n",
        "    unique_labels.add((label, class_name))\n",
        "\n",
        "# Print the unique labels and class names\n",
        "for label, class_name in unique_labels:\n",
        "    print(f\"Encoded label: {label}, Class name: {class_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5clj59Ayc6nS",
        "outputId": "6f7a28d0-5867-4ee3-b7d3-87e79f6f7585"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded label: 9, Class name: YFIB_YJAB_20_id90.fas\n",
            "Encoded label: 2, Class name: MLAB_PTSO_20_id90.fas\n",
            "Encoded label: 8, Class name: YEFM_YOEB_20_id90.fas\n",
            "Encoded label: 5, Class name: TESA_THIO_20_id90.fas\n",
            "Encoded label: 0, Class name: CSPD_IF1_20_id90.fas\n",
            "Encoded label: 3, Class name: RNPA_YBCJ_20_id90.fas\n",
            "Encoded label: 7, Class name: YDBL_YNBE_20_id90.fas\n",
            "Encoded label: 6, Class name: YAGP_YAHO_20_id90.fas\n",
            "Encoded label: 4, Class name: SLYX_TUSB_20_id90.fas\n",
            "Encoded label: 1, Class name: ISCA_ISCR_20_id90.fas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "skJwf5q9t4U_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the dataset\n",
        "train_dataset = SequenceDataset(train_encodings, train_labels)\n",
        "val_dataset = SequenceDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "pu5bwmMft54i"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear(pooled_output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "iQTNViC2uAQA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create an instance of the model and move it to the device\n",
        "model = MyModel(num_classes=len(label_encoder.classes_)).to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)  # Adjust learning rate if needed\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_accuracy = 0.0  # Track the best validation accuracy\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLxiwk9VuE93",
        "outputId": "c017e573-5825-4b46-d365-c02477cc76e0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7swO7d7NbKFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f66a57c-ebd2-43c2-a798-646f663f6997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "Train Loss: 1.7370 | Train Acc: 45.57%\n",
            "Validation Loss: 1.3136 | Validation Acc: 63.25%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YEFM_YOEB_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 2/10:\n",
            "Train Loss: 1.1305 | Train Acc: 67.52%\n",
            "Validation Loss: 0.9594 | Validation Acc: 72.52%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YEFM_YOEB_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 3/10:\n",
            "Train Loss: 0.7650 | Train Acc: 81.77%\n",
            "Validation Loss: 0.6150 | Validation Acc: 88.74%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 4/10:\n",
            "Train Loss: 0.4779 | Train Acc: 93.79%\n",
            "Validation Loss: 0.4013 | Validation Acc: 91.72%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 5/10:\n",
            "Train Loss: 0.3177 | Train Acc: 95.11%\n",
            "Validation Loss: 0.2864 | Validation Acc: 94.04%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 6/10:\n",
            "Train Loss: 0.2202 | Train Acc: 96.35%\n",
            "Validation Loss: 0.2493 | Validation Acc: 93.05%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 7/10:\n",
            "Train Loss: 0.2005 | Train Acc: 95.44%\n",
            "Validation Loss: 0.1963 | Validation Acc: 94.37%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 8/10:\n",
            "Train Loss: 0.1300 | Train Acc: 97.51%\n",
            "Validation Loss: 0.1626 | Validation Acc: 95.36%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 9/10:\n",
            "Train Loss: 0.1157 | Train Acc: 97.76%\n",
            "Validation Loss: 0.1594 | Validation Acc: 95.36%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n",
            "Epoch 10/10:\n",
            "Train Loss: 0.1004 | Train Acc: 97.68%\n",
            "Validation Loss: 0.1768 | Validation Acc: 95.03%\n",
            "Sample Predictions:\n",
            "Predicted: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'YAGP_YAHO_20_id90.fas']\n",
            "Target: ['RNPA_YBCJ_20_id90.fas' 'YDBL_YNBE_20_id90.fas' 'YEFM_YOEB_20_id90.fas'\n",
            " 'RNPA_YBCJ_20_id90.fas' 'SLYX_TUSB_20_id90.fas']\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2%}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_accuracy:.2%}\")\n",
        "\n",
        "    # Save the model if it has the best validation accuracy so far\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        best_val_accuracy = val_accuracy\n",
        "\n",
        "    # Debugging: Print some predictions and targets\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_inputs = next(iter(val_loader))\n",
        "        input_ids = sample_inputs['input_ids'].to(device)\n",
        "        attention_mask = sample_inputs['attention_mask'].to(device)\n",
        "        labels = sample_inputs['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "        # Convert label indices to class labels\n",
        "        predicted_classes = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
        "        target_classes = label_encoder.inverse_transform(labels.cpu().numpy())\n",
        "\n",
        "        print(\"Sample Predictions:\")\n",
        "        print(\"Predicted:\", predicted_classes[:5])\n",
        "        print(\"Target:\", target_classes[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/NEW_model_with_dash.pt'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ],
      "metadata": {
        "id": "xX85xp3Ij7sE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28c8e94-c5e5-4570-f55a-e5204088a5dc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Random sequence to predict\n",
        "random_sequence = \"-------------------------PYPQIVLFGDSLFQGCAHVDGFSFQASLQCHVMRRFDVVNRGFSGWNTANALKYLPDIIAPPQLKYLLVLLGANDAVTGVPLAEYKQNLLKIVTHPNITAHKPLVTPP--PIDTGAKISAEYTQAARDVAAEVPVTLIDLWAALHPGGAAL-LPDGLHMSGEGYKVFYKIVVPHIGQEY---------VPLTAEKFQTLVTMTQDPWFVKFYAPWCHHCQAMAPNWQQLAKEMKGKLNIGEVNCDVESRLCKDVRLRGYPTILFFKGGERV-EYDGLRGLGDFVHYAEKA--\"\n",
        "# Tokenize and encode the input sequence\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(random_sequence, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Move the input tensors to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# Load your trained model\n",
        "model = MyModel(num_classes=10)  # Replace with your trained model definition\n",
        "model.load_state_dict(torch.load(model_path))  # Replace with the path to your trained model\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize and fit the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_labels)  # Replace `train_labels` with the training labels used during training\n",
        "\n",
        "# Create a dictionary mapping class numbers to class names\n",
        "class_names = label_encoder.classes_\n",
        "class_number_to_name = {class_number: class_name for class_number, class_name in enumerate(class_names)}\n",
        "\n",
        "# Perform the prediction\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "# Convert the predicted label index to the original class name\n",
        "predicted_class = class_number_to_name[predicted_label]\n",
        "\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "V2UT17JSjcNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79ece4c-a999-4e7a-aaae-f0aa932f7e60"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4aGhSSKKAc_"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}