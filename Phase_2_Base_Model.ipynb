{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 2\n",
        "This code incorporates the modifications to the SequenceDataset class and tokenization process, enabling the model to handle combined inputs of sequence and property vectors, with the [L] token separating them."
      ],
      "metadata": {
        "id": "DZILB7NRs8YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "# install this module for extracting info from fas file instead of doing by hand\n",
        "!pip install biopython"
      ],
      "metadata": {
        "id": "n53bni69syQV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from Bio import SeqIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "8HuBoy_Us2Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the sequence DataFrame from the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Sequence DataFrame.csv')\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the sequences\n",
        "train_encodings = tokenizer(train_df['seq'].tolist(), truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_df['seq'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "# Prepare the property vectors\n",
        "train_property_vectors = train_df['property'].tolist()\n",
        "val_property_vectors = val_df['property'].tolist()\n"
      ],
      "metadata": {
        "id": "Xfap4japtHID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df['class or file name'].tolist())\n",
        "val_labels = label_encoder.transform(val_df['class or file name'].tolist())"
      ],
      "metadata": {
        "id": "76_IPX46tN9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels, property_vectors):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.property_vectors = property_vectors\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.encodings['input_ids'][idx]\n",
        "        attention_mask = self.encodings['attention_mask'][idx]\n",
        "        property_vector = self.property_vectors[idx]\n",
        "\n",
        "        # Combine sequence and property with [L] token\n",
        "        combined_input = input_ids + [tokenizer.token_to_id('[L]')] + property_vector\n",
        "\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(combined_input),\n",
        "            'attention_mask': torch.tensor(attention_mask),\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "tGPmz9GgtTVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the dataset\n",
        "train_dataset = SequenceDataset(train_encodings, train_labels, train_property_vectors)\n",
        "val_dataset = SequenceDataset(val_encodings, val_labels, val_property_vectors)"
      ],
      "metadata": {
        "id": "3DqLXDzotUMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.linear(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "_P1Osw6OtX1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to use\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create an instance of the model and move it to the device\n",
        "model = MyModel(num_classes=len(label_encoder.classes_)).to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "G7ndEXK-tbhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU6Xptecst-U"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = train_correct / train_total\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2%}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Acc: {val_accuracy:.2%}\")\n"
      ]
    }
  ]
}